{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User routes on the site\n",
    "## Description\n",
    "**Clickstream** is a sequence of user actions on a website. It allows you to understand how users interact with the site. In this task, you need to find the most frequent custom routes.\n",
    "\n",
    "## Input data\n",
    "Input data is а table with clickstream data in file `hdfs:/data/clickstream.csv`.\n",
    "\n",
    "### Table structure\n",
    "* `user_id (int)` - Unique user identifier.\n",
    "* `session_id (int)` - Unique identifier for the user session. The user's session lasts until the identifier changes.\n",
    "* `event_type (string)` - Event type from the list:\n",
    "    * **page** - visit to the page\n",
    "    * **event** - any action on the page\n",
    "    * <b>&lt;custom&gt;</b> - string with any other type\n",
    "* `event_type (string)` - Page on the site.\n",
    "* `timestamp (int)` - Unix-timestamp of action.\n",
    "\n",
    "### Browser errors\n",
    "Errors can sometimes occur in the user's browser - after such an error appears, we can no longer trust the data of this session and all the following lines after the error or at the same time with it are considered corrupted and **should not be counted** in statistics.\n",
    "\n",
    "When an error occurs on the page, a random string containing the word **error** will be written to the `event_type` field.\n",
    "\n",
    "### Sample of user session\n",
    "<pre>\n",
    "+-------+----------+------------+----------+----------+\n",
    "|user_id|session_id|  event_type|event_page| timestamp|\n",
    "+-------+----------+------------+----------+----------+\n",
    "|    562|       507|        page|      main|1620494781|\n",
    "|    562|       507|       event|      main|1620494788|\n",
    "|    562|       507|       event|      main|1620494798|\n",
    "|    562|       507|        page|    family|1620494820|\n",
    "|    562|       507|       event|    family|1620494828|\n",
    "|    562|       507|        page|      main|1620494848|\n",
    "|    562|       507|wNaxLlerrorU|      main|1620494865|\n",
    "|    562|       507|       event|      main|1620494873|\n",
    "|    562|       507|        page|      news|1620494875|\n",
    "|    562|       507|        page|   tariffs|1620494876|\n",
    "|    562|       507|       event|   tariffs|1620494884|\n",
    "|    562|       514|        page|      main|1620728918|\n",
    "|    562|       514|       event|      main|1620729174|\n",
    "|    562|       514|        page|   archive|1620729674|\n",
    "|    562|       514|        page|     bonus|1620729797|\n",
    "|    562|       514|        page|   tariffs|1620731090|\n",
    "|    562|       514|       event|   tariffs|1620731187|\n",
    "+-------+----------+------------+----------+----------+\n",
    "</pre>\n",
    "\n",
    "#### Correct user routes for a given user:\n",
    "* **Session 507**: main-family-main\n",
    "* **Session 514**: main-archive-bonus-tariffs\n",
    "\n",
    "Route elements are ordered by the time they appear in the clickstream, from earliest to latest.\n",
    "\n",
    "The route must be accounted for completely before the end of the session or an error in the session.\n",
    "\n",
    "## Task\n",
    "You need to use the Spark SQL, Spark RDD and Spark DF interfaces to create a solution file, the lines of which contain **the 30 most frequent user routes** on the site.\n",
    "\n",
    "Each line of the file should contain the `route` and `count` values **separated by tabs**, where:\n",
    "* `route` - route on the site, consisting of pages separated by \"-\".\n",
    "* `count` - the number of user sessions in which this route was.\n",
    "\n",
    "The lines must be **ordered in descending order** of the `count` field.\n",
    "\n",
    "## Criteria\n",
    "You can get maximum of 3.5 points (final grade) for this assignment, depedning on the number of interface you manage to leverage. The criteria are as follows:\n",
    "\n",
    "* 0.5 points – Spark SQL solution with 1 query\n",
    "* 0.5 points – Spark SQL solution with <=2 queries\n",
    "* 0.5 points – Spark RDD solution\n",
    "* 0.5 points – Spark DF solution\n",
    "* 0.5 points – your solution algorithm is relatively optimized, i.e.: no O^2 or O^3 complexities; appropriate object usage; no data leaks etc. This is evaluated by staff.\n",
    "* 1 point – 1 on 1 screening session. During this session staff member can ask you questions regarding your solution logic, framework usage, questionable parts of your code etc. If your code is clean enough, the staff member can just ask you to solve a theoretical problem connected to Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/conda/lib/python3.10/site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark\n",
      "Version: 3.5.0\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: /opt/conda/lib/python3.10/site-packages\n",
      "Requires: py4j\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2023-10-10 15:59:08,787 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName='jupyter')\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "se = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clickstream.csv\t\t\t   sga-task.ipynb\n",
      "hdfs-task.ipynb\t\t\t   sga-task-ndzhassal2.ipynb\n",
      "LSML_ClickStreams_SGA.ipynb\t   sga-task-ndzhassal2-Version2.ipynb\n",
      "ndzhassal2_spark_sql_solution.csv  spark-warehouse\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/clickstream.csv': File exists\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -copyFromLocal clickstream.csv /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import concat_ws, collect_list\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import Window\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = se.read.csv(\"hdfs:///clickstream.csv\", header=True, inferSchema=True, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+----------+----------+\n",
      "|user_id|session_id|event_type|event_page| timestamp|\n",
      "+-------+----------+----------+----------+----------+\n",
      "|   4645|       935|      page|   archive|1696270588|\n",
      "|   2251|       378|      page|    rabota|1696270591|\n",
      "|   2222|       704|      page|    rabota|1696270600|\n",
      "|   2366|       339|      page|     vklad|1696270605|\n",
      "|   2233|       165|     event|    rabota|1696270605|\n",
      "|   3253|       528|     event|      news|1696270612|\n",
      "|    109|       433|     event|     bonus|1696270613|\n",
      "|   4367|       106|     event|  internet|1696270619|\n",
      "|   2222|       704|     event|    rabota|1696270628|\n",
      "|   4304|       640|     event|  internet|1696270636|\n",
      "+-------+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary view of the data\n",
    "data.createOrReplaceTempView(\"ClickStreams\")\n",
    "data.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- session_id: integer (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- event_page: string (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+----------+----------+\n",
      "|user_id|session_id|  event_type|event_page| timestamp|\n",
      "+-------+----------+------------+----------+----------+\n",
      "|    562|       507|        page|      main|1695584127|\n",
      "|    562|       507|       event|      main|1695584134|\n",
      "|    562|       507|       event|      main|1695584144|\n",
      "|    562|       507|       event|      main|1695584147|\n",
      "|    562|       507|wNaxLlerrorU|      main|1695584154|\n",
      "|    562|       507|       event|      main|1695584154|\n",
      "|    562|       507|       event|      main|1695584154|\n",
      "|    562|       507|       event|      main|1695584160|\n",
      "|    562|       507|        page|    rabota|1695584166|\n",
      "|    562|       507|       event|    rabota|1695584174|\n",
      "+-------+----------+------------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Data Types: [('user_id', 'int'), ('session_id', 'int'), ('event_type', 'string'), ('event_page', 'string'), ('timestamp', 'int')]\n",
      "There are 1000000 rows in the ClickStreams table\n"
     ]
    }
   ],
   "source": [
    "data.show(10)\n",
    "print(\"Data Types:\", data.dtypes)\n",
    "print(\"There are {0} rows in the ClickStreams table\".format(data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Spark SQL (1.0 marks)\n",
    "\n",
    "- 0.5 points – Spark SQL solution with 1 query\n",
    "- 0.5 points – Spark SQL solution with <=2 queries\n",
    "\n",
    "## Task: Use the Spark SQL interface to create a solution file, the lines of which should contain **the 30 most frequent user routes** on the site. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               route|count|\n",
      "+--------------------+-----+\n",
      "|                main| 8090|\n",
      "|        main-archive| 1088|\n",
      "|         main-rabota| 1037|\n",
      "|       main-internet|  879|\n",
      "|          main-bonus|  865|\n",
      "|           main-news|  759|\n",
      "|        main-tariffs|  669|\n",
      "|         main-online|  584|\n",
      "|          main-vklad|  512|\n",
      "| main-archive-rabota|  167|\n",
      "| main-rabota-archive|  167|\n",
      "|  main-bonus-archive|  139|\n",
      "|   main-rabota-bonus|  136|\n",
      "|    main-news-rabota|  134|\n",
      "|   main-bonus-rabota|  133|\n",
      "|main-archive-inte...|  131|\n",
      "|    main-rabota-news|  129|\n",
      "|main-internet-rabota|  128|\n",
      "|   main-archive-news|  124|\n",
      "|main-rabota-internet|  123|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_sql_solution = se.sql(\"\"\"\n",
    "SELECT CONCAT_WS('-', route_list) AS route, COUNT(*) AS count\n",
    "FROM (\n",
    "    SELECT user_id, session_id, COLLECT_LIST(event_page) AS route_list \n",
    "    FROM (\n",
    "        SELECT cs.user_id, cs.session_id, cs.event_page, cs.timestamp \n",
    "        FROM ClickStreams cs\n",
    "        WHERE cs.event_type = 'page' AND NOT EXISTS (\n",
    "            SELECT 1 FROM ClickStreams c\n",
    "            WHERE c.user_id = cs.user_id AND c.session_id = cs.session_id AND c.event_type LIKE '%error%' AND c.timestamp < cs.timestamp\n",
    "        )\n",
    "    ) user_routes \n",
    "    GROUP BY user_id, session_id\n",
    ") user_routes\n",
    "GROUP BY route\n",
    "ORDER BY count DESC\n",
    "LIMIT 30;\"\"\")\n",
    "\n",
    "spark_sql_solution.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save to csv\n",
    "spark_sql_solution.toPandas().to_csv(\"ndzhassal2_spark_sql_output.csv\", index=False, header=True, sep = \"\\t\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Part_1 = pd.read_csv(\"ndzhassal2_spark_sql_output.csv\", sep = \"\\t\")\n",
    "len(Part_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Spark RDD (0.5 marks)\n",
    "\n",
    "- 0.5 points – Spark RDD solution\n",
    "\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions\n",
    "\n",
    "## Task: Use the Spark RDD interface to create a solution file, the lines of which contain **the 30 most frequent user routes** on the site.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdd_data = data.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Filter 'error' events and map them to tuples in format: ((user_id, session_id), timestamp)\n",
    "error_tuples = rdd_data.filter(lambda r: 'error' in r.event_type) \\\n",
    "                       .map(lambda r: ((r.user_id, r.session_id), r.timestamp))\n",
    "\n",
    "# Use reduceByKey() to find the minimum timestamp for each session\n",
    "errors = error_tuples.reduceByKey(lambda a, b: a if a < b else b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2: Filter 'page' events and map them to tuples in format: ((user_id, session_id), event)\n",
    "page_tuples = rdd_data.filter(lambda r: r.event_type == 'page') \\\n",
    "                      .map(lambda r: ((r.user_id, r.session_id), r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 3: Join the two RDDs to get tuples in format: ((user_id, session_id), (event, error_timestamp)) \n",
    "joined_tuples = page_tuples.leftOuterJoin(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 4: Filter out tuples that contain errors or where event_type = 'page' occurs after an error.\n",
    "clean_tuples = joined_tuples.filter(lambda x: x[1][1] is None or x[1][0].timestamp < x[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 5: Mapping clean_tuples to tuples in format: ((user_id, session_id), [event_page])\n",
    "session_events = clean_tuples.map(lambda x: (x[0], [x[1][0].event_page])) \\\n",
    "                             .reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 6: \n",
    "# Mapping session_events to add '-' between events \n",
    "# Using reduceByKey() to count number of occurrences of each route\n",
    "user_route_counts = session_events.map(lambda x: (\"-\".join(x[1]), 1))\n",
    "user_route_counts = user_route_counts.reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 7: # Sort route_counts by count in descending order and take the top 30 results\n",
    "top_routes = user_route_counts.takeOrdered(30, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(top_routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_file = \"ndzhassal2_spark_RDD_output.csv\"\n",
    "cols = [\"route\", \"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(output_file, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(cols)\n",
    "    writer.writerows(top_routes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Spark DF (0.5 marks)\n",
    "\n",
    "- 0.5 points – Spark DF solution\n",
    "\n",
    "## Task: Use the Spark DF interface to create a solution file, the lines of which contain **the 30 most frequent user routes** on the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before conversion: <class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Before conversion:\", type(rdd_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverting rdd_data to Spark DataFrame\n",
    "clickstream_df = rdd_data.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After conversion: <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(\"After conversion:\", type(clickstream_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+----------+----------+\n",
      "|user_id|session_id|  event_type|event_page| timestamp|\n",
      "+-------+----------+------------+----------+----------+\n",
      "|    562|       507|        page|      main|1695584127|\n",
      "|    562|       507|       event|      main|1695584134|\n",
      "|    562|       507|       event|      main|1695584144|\n",
      "|    562|       507|       event|      main|1695584147|\n",
      "|    562|       507|wNaxLlerrorU|      main|1695584154|\n",
      "+-------+----------+------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clickstream_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- session_id: long (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- event_page: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clickstream_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Filter errors and calculate min_timestamp\n",
    "errors_df = (\n",
    "    clickstream_df\n",
    "    .filter(F.col('event_type').like('%error%'))  \n",
    "    .groupBy('user_id', 'session_id')  \n",
    "    .agg(F.min('timestamp').alias('min_timestamp'))  # Calculate minimum value of timestamp for each group\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2: Join tables and filter data\n",
    "user_sessions_df = (\n",
    "    clickstream_df\n",
    "    .join(errors_df, ['user_id', 'session_id'], how='left')  # Join with errors_df on user_id and session_id columns\n",
    "    .filter(((F.col('timestamp') < F.col('min_timestamp')) | F.col('min_timestamp').isNull()) & (F.col('event_type') == 'page'))  # Keep rows where timestamp is less than min_timestamp or min_timestamp is NULL, and event_type is \"page\"\n",
    "    .withColumn('rn', F.row_number().over(Window.partitionBy('user_id', 'session_id').orderBy(F.col('timestamp').desc())))  # Add a new column \"rn\" that assigns a row number to each row within each group, based on the descending order of the timestamp column\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 3: Collect user routes and filter data\n",
    "filtered_user_routes_df = (\n",
    "    user_sessions_df\n",
    "    .groupBy('user_id', 'session_id')\n",
    "    .agg(F.collect_list('event_page').alias('route'), F.first('timestamp').alias('max_timestamp'))  \n",
    "    .withColumn('rn', F.row_number().over(Window.partitionBy('user_id', 'session_id').orderBy(F.col('max_timestamp').desc())))  \n",
    "    .filter(F.col('rn') == 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               route|count|\n",
      "+--------------------+-----+\n",
      "|              [main]| 8090|\n",
      "|     [main, archive]| 1094|\n",
      "|      [main, rabota]| 1037|\n",
      "|    [main, internet]|  880|\n",
      "|       [main, bonus]|  865|\n",
      "|        [main, news]|  760|\n",
      "|     [main, tariffs]|  669|\n",
      "|      [main, online]|  584|\n",
      "|       [main, vklad]|  514|\n",
      "|[main, archive, r...|  167|\n",
      "|[main, rabota, ar...|  167|\n",
      "|[main, bonus, arc...|  139|\n",
      "|[main, rabota, bo...|  137|\n",
      "|[main, news, rabota]|  134|\n",
      "|[main, bonus, rab...|  133|\n",
      "|[main, archive, i...|  131|\n",
      "|[main, rabota, news]|  129|\n",
      "|[main, internet, ...|  128|\n",
      "|[main, archive, n...|  124|\n",
      "|[main, internet, ...|  123|\n",
      "|[main, rabota, in...|  123|\n",
      "|[main, archive, b...|  117|\n",
      "|[main, internet, ...|  114|\n",
      "|[main, tariffs, i...|  114|\n",
      "|[main, news, arch...|  112|\n",
      "|[main, news, inte...|  108|\n",
      "|[main, archive, t...|  103|\n",
      "|[main, tariffs, a...|  102|\n",
      "|[main, internet, ...|  101|\n",
      "|        [main, main]|   94|\n",
      "+--------------------+-----+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Group by routes and count\n",
    "all_routes = (\n",
    "    filtered_user_routes_df\n",
    "    .groupBy('route')\n",
    "    .agg(F.count('*').alias('count'))\n",
    "    .orderBy('count', ascending=False)  # Sort in descending order\n",
    ")\n",
    "\n",
    "all_routes.show(30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(all_routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_file = \"ndzhassal2_spark_DF_output.csv\"\n",
    "cols = [\"route\", \"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "with open(output_file, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(cols)\n",
    "    writer.writerows(all_routes.collect()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "week-4-spark-homework"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
